{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens in SpaCy.\n",
    "In spaCy, a \"token\" refers to an individual piece of a text, like a word or a punctuation mark. When spaCy processes a text, it splits it into tokens, which is a process known as tokenization. This is a fundamental step in Natural Language Processing (NLP), as it breaks down text into manageable pieces for further analysis.\n",
    "\n",
    "The process of tokenization breaks a text down into its basic units—or tokens—which are represented in spaCy as Token objects.\n",
    "\n",
    "Each token in spaCy is an instance of the Token class, which contains various attributes and methods to access linguistic features or metadata about the token. For example, for each token, you can get its text, lemma (base form), part-of-speech tag, dependency relation to other tokens, and whether it is a stop word, among other properties.\n",
    "\n",
    "This tokenization forms the basis for more complex NLP tasks performed by spaCy, such as parsing, named entity recognition, and more. By breaking text into tokens, spaCy allows for a more detailed and nuanced analysis of the text's structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus Proto is a Python developer currently working for a London-based Fintech company. He is interested in learning Natural Language Processing.\n"
     ]
    }
   ],
   "source": [
    "about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "about_doc = nlp(about_text)\n",
    "print(about_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus 0\n",
      "Proto 4\n",
      "is 10\n",
      "a 13\n",
      "Python 15\n",
      "developer 22\n",
      "currently 32\n",
      "working 42\n",
      "for 50\n",
      "a 54\n",
      "London 56\n",
      "- 62\n",
      "based 63\n",
      "Fintech 69\n",
      "company 77\n",
      ". 84\n",
      "He 86\n",
      "is 89\n",
      "interested 92\n",
      "in 103\n",
      "learning 106\n",
      "Natural 115\n",
      "Language 123\n",
      "Processing 132\n",
      ". 142\n"
     ]
    }
   ],
   "source": [
    "# Now, let print the tokens and their indexes in the string.\n",
    "# this will done by iterating over the tokens in the Doc.\n",
    "# The token.idx attribute returns the token's character offset in the Doc.\n",
    "\n",
    "for token in about_doc:\n",
    "    print(token, token.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
